{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importação de bibliotecas"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variáveis parâmetros"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SystemPath = 'TI/XLS_CVS_teste'\r\n",
        "FileName = 'XLS_CVS_teste'\r\n",
        "DatePath = '2023/10/16/07/00'"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool2",
              "session_id": "44504",
              "statement_id": 2,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-10-16T10:05:35.4229077Z",
              "session_start_time": "2023-10-16T10:05:35.4938356Z",
              "execution_start_time": "2023-10-16T10:09:07.8728385Z",
              "execution_finish_time": "2023-10-16T10:09:08.0408193Z",
              "spark_jobs": null,
              "parent_msg_id": "a97942c1-72ee-4e7f-a10a-a706ec30edae"
            },
            "text/plain": "StatementMeta(SparkPool2, 44504, 2, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import split, col, row_number, expr, row_number, current_timestamp\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "\r\n",
        "from pyspark.sql.window import Window\r\n",
        "\r\n",
        "from pyspark import HiveContext\r\n",
        "from pyspark.sql import Row, functions as F, types as T\r\n",
        "from pyspark.sql.window import Window\r\n",
        "\r\n",
        "from pyspark.sql.functions import col, lit, split, input_file_name, substring, regexp_replace, trim, when, add_months, to_date, concat, coalesce, months_between, max, sum, count, avg\r\n",
        "\r\n",
        "from datetime import datetime, timedelta\r\n",
        "\r\n",
        "from functools import reduce"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool2",
              "session_id": "44504",
              "statement_id": 3,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-10-16T10:09:07.5559405Z",
              "session_start_time": null,
              "execution_start_time": "2023-10-16T10:09:08.2088291Z",
              "execution_finish_time": "2023-10-16T10:09:08.4211166Z",
              "spark_jobs": null,
              "parent_msg_id": "fe992438-a62d-440a-a1db-8af319a5df2f"
            },
            "text/plain": "StatementMeta(SparkPool2, 44504, 3, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Áreas de Dados"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LZ_path = 'abfss://landing-zone@romagnoledatalake1.dfs.core.windows.net/'+ SystemPath +'/'+ DatePath +'/' + FileName +'.avro'\r\n",
        "CZ_path = 'abfss://consume-zone@romagnoledatalake1.dfs.core.windows.net/'+ SystemPath"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool2",
              "session_id": "44504",
              "statement_id": 4,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-10-16T10:09:25.8007651Z",
              "session_start_time": null,
              "execution_start_time": "2023-10-16T10:09:25.9946203Z",
              "execution_finish_time": "2023-10-16T10:09:26.2171881Z",
              "spark_jobs": null,
              "parent_msg_id": "9d8525a8-7863-4b81-b532-9447855bd464"
            },
            "text/plain": "StatementMeta(SparkPool2, 44504, 4, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Leitura do arquivo CSV\r\n",
        "# Estrutura original: \r\n",
        "#PRODUTO;ANO;REGIAO;ESTADO;Jan;Fev;Mar;Abr;Mai;Jun;Jul;Ago;Set;Out;Nov;Dez;TOTAL\r\n",
        "#GASOLINA C (m3);2000;REGIÃO NORTE;RONDÔNIA;136073,253;9563,263;11341,229;9369,746;10719,983;11165,968;12312,451;11220,97;12482,281;13591,122;11940,57;11547,576;10818,094\r\n",
        "#GASOLINA C (m3);2000;REGIÃO NORTE;ACRE;3358,346;40001,853;3065,758;3495,29;2946,93;3023,92;3206,93;3612,58;3264,46;3835,74;3676,571;3225,61;3289,718\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.load('abfss://landing-zone@romagnoledatalake1.dfs.core.windows.net/z_Lab/anp_prod_ok.csv', format='csv', header=True, sep=\";\")\r\n",
        "colunas_a_manter = ['product', 'year', 'unit', 'uf']\r\n",
        "#realizando pivot das colunas de Meses para Linhar\r\n",
        "df_melted = df.select(colunas_a_manter + [expr(\"stack(12, 'JAN', JAN, 'FEV', FEV, 'MAR', MAR, 'ABR', ABR, 'MAI', MAI, 'JUN', JUN, 'JUL', JUL, 'AGO', AGO, 'SET', SET, 'OUT', OUT, 'NOV', NOV, 'DEZ', DEZ) as (month, volume)\")])\r\n",
        "#tipagem de campo e geração de campo Mês numérico (para ordenação)\r\n",
        "df = df_melted\\\r\n",
        "    .withColumn('year', col('year').cast('int'))\\\r\n",
        "    .withColumn('volume',when(col('volume').isNotNull(), col('volume')).otherwise(0).cast('float'))\\\r\n",
        "    .withColumn('created_at',current_timestamp())\\\r\n",
        "    .withColumn('month_no',\\\r\n",
        "        when(col('month') == 'JAN', 1).\\\r\n",
        "        when(col('month') == 'FEV', 2).\\\r\n",
        "        when(col('month') == 'MAR', 3).\\\r\n",
        "        when(col('month') == 'ABR', 4).\\\r\n",
        "        when(col('month') == 'MAI', 5).\\\r\n",
        "        when(col('month') == 'JUN', 6).\\\r\n",
        "        when(col('month') == 'JUL', 7).\\\r\n",
        "        when(col('month') == 'AGO', 8).\\\r\n",
        "        when(col('month') == 'SET', 9).\\\r\n",
        "        when(col('month') == 'OUT', 10).\\\r\n",
        "        when(col('month') == 'NOV', 11).\\\r\n",
        "        when(col('month') == 'DEZ', 12).\\\r\n",
        "        otherwise(0).cast('int'))     \r\n",
        "   \r\n",
        "#filtro de registros apenas com valores válidos\r\n",
        "df = df.filter(df['volume'].cast('string').rlike('^[0-9.]+$'))\r\n",
        "#display(df.limit(12)\r\n",
        "\r\n",
        "#gravar dados na Consume Zone\r\n",
        "df.write.mode('overwrite').save(CZ_path, format='parquet')\r\n",
        "\r\n",
        "\r\n",
        "df_pivot = df.groupBy(\"month\", \"month_no\").pivot(\"year\").agg(sum(\"volume\")).orderBy(\"month_no\")\r\n",
        "\r\n",
        "agg_cols = df_pivot.columns[1:]\r\n",
        "rollup_df = df_pivot.rollup().sum()\r\n",
        "\r\n",
        "renamed_df = reduce(\r\n",
        "    lambda rollup_df, idx: rollup_df.withColumnRenamed(rollup_df.columns[idx], agg_cols[idx]), \r\n",
        "    range(len(rollup_df.columns)), rollup_df\r\n",
        ")\r\n",
        "\r\n",
        "renamed_df = renamed_df.withColumn('month', lit('Total'))\r\n",
        "\r\n",
        "df_pivot.unionByName(\r\n",
        "    renamed_df\r\n",
        ")\r\n",
        "\r\n",
        "colunas = df_pivot.columns[2:-1]\r\n",
        "\r\n",
        "# Crie uma janela de especificação para a acumulação\r\n",
        "window_spec = Window.orderBy()  # Você pode especificar a ordem conforme necessário\r\n",
        "\r\n",
        "# Calcule a soma acumulativa das duas últimas colunas\r\n",
        "for coluna in colunas[-2:]:\r\n",
        "    df_pivot = df_pivot.withColumn(coluna + '_cumsum', sum(col(coluna)).over(window_spec))\r\n",
        "\r\n",
        "# Calcule a nova coluna acumulativa com base nas duas últimas colunas\r\n",
        "df_pivot = df_pivot.withColumn('variacao acumulado 19-20',\r\n",
        "                   when((col(colunas[-2] + '_cumsum') + col(colunas[-1] + '_cumsum') == 0), \"n/d\")\r\n",
        "                   .otherwise((col(colunas[-1] + '_cumsum') / col(colunas[-2] + '_cumsum') - 1) * 100))\r\n",
        "\r\n",
        "# Mostre o DataFrame resultante\r\n",
        "df_pivot.show()\r\n",
        "\r\n",
        "#display(final_df.limit(12))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool2",
              "session_id": "44504",
              "statement_id": 5,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-10-16T10:09:36.0942729Z",
              "session_start_time": null,
              "execution_start_time": "2023-10-16T10:09:36.2870638Z",
              "execution_finish_time": "2023-10-16T10:10:28.7209539Z",
              "spark_jobs": null,
              "parent_msg_id": "703d0877-801c-484f-bcf0-155de0091590"
            },
            "text/plain": "StatementMeta(SparkPool2, 44504, 5, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------+-------+-------+------+-------+-------+-------+-------+--------+------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---------+--------+-----------+-----------+------------------------+\n|month|month_no|   2000|   2001|  2002|   2003|   2004|   2005|   2006|    2007|  2008|    2009|    2010|    2011|    2012|    2013|    2014|    2015|    2016|    2017|    2018|     2019|    2020|2018_cumsum|2019_cumsum|variacao acumulado 19-20|\n+-----+--------+-------+-------+------+-------+-------+-------+-------+--------+------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---------+--------+-----------+-----------+------------------------+\n|  JAN|       1|43251.0|  758.0|5939.0|  880.0| 1679.0| 1012.0| 8621.0|  2550.0| 685.0| 74209.0| 34240.0| 15965.0|183658.0|202204.0|374138.0|271489.0|182770.0|174703.0|  7082.0| 250973.0|209831.0|   769254.0|  4139437.0|       438.1105590611163|\n|  FEV|       2|26507.0| 1604.0|3827.0| 1061.0|27589.0|  890.0|10070.0|  1305.0| 543.0| 72013.0| 35774.0| 47786.0|135492.0|321366.0|254370.0|173768.0|463974.0| 42014.0| 14107.0| 236201.0|161866.0|   769254.0|  4139437.0|       438.1105590611163|\n|  MAR|       3| 1556.0| 1380.0|3169.0| 4604.0|16416.0|19030.0| 4797.0|  2084.0|1462.0|128022.0| 52141.0|171534.0|144465.0|312276.0|780722.0|285645.0|182854.0|116699.0| 76773.0| 238141.0|200025.0|   769254.0|  4139437.0|       438.1105590611163|\n|  ABR|       4| 1070.0|  874.0|5054.0| 2263.0| 3582.0| 1623.0| 4447.0|  1013.0|1263.0| 34393.0| 27324.0|132510.0|123072.0|375503.0|254061.0|452728.0|206713.0| 47503.0|106274.0|1510830.0|165305.0|   769254.0|  4139437.0|       438.1105590611163|\n|  MAI|       5| 1249.0| 1411.0| 928.0| 3024.0|12809.0|16896.0| 5724.0|  2052.0|1153.0| 15206.0|163140.0|129007.0| 94438.0|359912.0|187381.0|164756.0|167375.0|398509.0| 60263.0| 246525.0| 93114.0|   769254.0|  4139437.0|       438.1105590611163|\n|  JUN|       6|  194.0| 1314.0|4854.0|  963.0| 2154.0| 4054.0| 8008.0|  2152.0|3188.0| 78864.0| 85494.0|150859.0|139823.0|281899.0|246826.0|139830.0|273581.0| 48609.0| 38315.0| 288656.0| 97913.0|   769254.0|  4139437.0|       438.1105590611163|\n|  JUL|       7| 2630.0| 1359.0| 945.0| 1174.0| 1865.0| 1613.0|16099.0|  2482.0|2587.0|102018.0|157505.0|  9212.0|121243.0|250189.0| 96884.0|122885.0|173065.0| 99561.0| 50310.0| 424976.0|158712.0|   769254.0|  4139437.0|       438.1105590611163|\n|  AGO|       8|  324.0| 1448.0|1197.0| 1319.0| 1703.0|32553.0| 5303.0|135620.0|2361.0| 86590.0| 64704.0| 92922.0|128197.0|276628.0|215004.0|380423.0|645445.0| 13608.0|170378.0| 210451.0|131211.0|   769254.0|  4139437.0|       438.1105590611163|\n|  SET|       9|  337.0|17489.0|1254.0| 1245.0| 1845.0| 1392.0| 6192.0|  1056.0|2457.0| 11722.0| 18680.0| 37092.0| 44352.0|342003.0|554809.0|147493.0| 99652.0|263162.0| 43728.0| 223831.0|101435.0|   769254.0|  4139437.0|       438.1105590611163|\n|  OUT|      10| 2860.0| 2178.0|8982.0|23563.0|14939.0| 1561.0|  928.0|  3069.0|1115.0| 72848.0|107917.0|140029.0| 33961.0|316426.0|291677.0|249904.0|233366.0| 32768.0| 61499.0| 104765.0|236224.0|   769254.0|  4139437.0|       438.1105590611163|\n|  NOV|      11|  943.0|  978.0|1366.0| 1991.0| 7217.0| 4083.0|  842.0|  1281.0|1000.0|  8461.0| 97346.0| 16417.0| 58013.0|284041.0|259999.0|152671.0|143069.0|173527.0| 16093.0| 116589.0|183921.0|   769254.0|  4139437.0|       438.1105590611163|\n|  DEZ|      12| 9029.0| 1821.0|2239.0| 6031.0| 5507.0|14336.0| 3367.0|  3105.0|1362.0| 12610.0|120302.0| 43212.0|109730.0|427648.0|237637.0|275263.0|487143.0|189267.0|124432.0| 287499.0|156385.0|   769254.0|  4139437.0|       438.1105590611163|\n+-----+--------+-------+-------+------+-------+-------+-------+-------+--------+------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---------+--------+-----------+-----------+------------------------+\n\n"
          ]
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {},
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resultados agrupados"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#agrupamento e sumarização de valor, por ESTADO, PRODUTO, ANO e MES\r\n",
        "#df_agregado = df.groupBy(\"uf\", \"product\", \"year\", \"month_no\", \"month\").agg(sum(\"volume\").alias(\"TOTAL\")).orderBy(\"uf\", \"product\", \"year\", \"month_no\")\r\n",
        "#display(df_agregado.limit(12))\r\n",
        "\r\n",
        "#agrupamento e sumarização de valor, por ESTADO, ANO e MES\r\n",
        "#df_agregado = df.groupBy(\"uf\", \"year\", \"month_no\", \"month\").agg(sum(\"volume\").alias(\"volume\")).orderBy(\"uf\", \"year\", \"month_no\")\r\n",
        "#display(df_agregado.limit(12))\r\n",
        "\r\n",
        "#agrupamento e sumarização de valor, por PRODUTO, ANO e MES\r\n",
        "#df_agregado = df.groupBy(\"product\", \"year\", \"month_no\", \"month\").agg(sum(\"volume\").alias(\"volume\")).orderBy(\"year\", \"month_no\", \"product\")\r\n",
        "#display(df_agregado.limit(12))\r\n",
        "\r\n",
        "\r\n",
        "#filtro por Produto = DIESEL e agrupamento e sumarização de valor, por PRODUTO, ANO e MES\r\n",
        "#df_diesel = df.filter(col(\"product\").like(\"%DIESEL%\"))\r\n",
        "#df_agregado = df_diesel.groupBy(\"product\", \"year\", \"month_no\", \"month\").agg(sum(\"volume\").alias(\"accum_total\")).orderBy(\"year\", \"month_no\", \"product\")\r\n",
        "#display(df_agregado.limit(12))\r\n",
        "\r\n",
        "\r\n",
        "# Use a função sum e over para calcular o total acumulado por mês e ano.\r\n",
        "#window_spec = Window.partitionBy(\"MES\").orderBy(\"ANO\").rowsBetween(Window.unboundedPreceding, 0)\r\n",
        "#df_totalizado = df.withColumn(\"accum_total\", sum(\"volume\").over(window_spec))\r\n",
        "#display(df_totalizado.limit(12))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%pyspark\r\n",
        "df = spark.read.load('abfss://landing-zone@romagnoledatalake1.dfs.core.windows.net/z_Lab/Relatório CPR091.csv', format='csv'\r\n",
        "## If header exists uncomment line below\r\n",
        ", header=True\r\n",
        ")\r\n",
        "display(df.limit(10))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "df = pd.read_excel('abfss://landing-zone@romagnoledatalake1.dfs.core.windows.net/z_Lab/Relatório CPR091.xls', engine='openpyxl')\r\n",
        "display(df.limit(10))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {}
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "df = pd.read_excel('abfss://landing-zone@romagnoledatalake1.dfs.core.windows.net/z_Lab/anp_LibreOffice.xlsx', engine='openpyxl')\r\n",
        "display(df.limit(10))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {},
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from colorama import Back, Fore, init\r\n",
        "from openpyxl import load_workbook\r\n",
        " \r\n",
        "ROWS, COLS = 10, 10\r\n",
        "WORKBOOK, WORKSHEET = \"abfss://landing-zone@romagnoledatalake1.dfs.core.windows.net/z_Lab/anp.xlsx\", \"Plan1\"\r\n",
        " \r\n",
        "wb = load_workbook(WORKBOOK)\r\n",
        "ws = wb[WORKSHEET]\r\n",
        " \r\n",
        "init()\r\n",
        " \r\n",
        "for r in range(1, ROWS + 1):\r\n",
        "    print(\r\n",
        "        Fore.CYAN if r % 2 else Fore.MAGENTA,\r\n",
        "        \" | \".join(\r\n",
        "            [\r\n",
        "                \"{:5d}\".format(\r\n",
        "                    ws.cell(row=r, column=c).value\r\n",
        "                )\r\n",
        "                for c in range(1, COLS + 1)\r\n",
        "            ]\r\n",
        "        ),\r\n",
        "        Fore.RESET, sep=\"\"\r\n",
        "    )"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}